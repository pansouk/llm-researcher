{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66894391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers(\"Language Models\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0125ac0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models',\n",
       " 'Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought',\n",
       " 'From Chat Logs to Collective Insights: Aggregative Question Answering',\n",
       " 'MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence',\n",
       " 'ZeroGUI: Automating Online GUI Learning at Zero Human Cost',\n",
       " 'Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch',\n",
       " 'Differential Information: An Information-Theoretic Perspective on Preference Optimization',\n",
       " 'Model Immunization from a Condition Number Perspective',\n",
       " \"Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint\",\n",
       " 'Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models',\n",
       " 'LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers',\n",
       " 'DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning',\n",
       " 'ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks',\n",
       " 'REOrdering Patches Improves Vision Models',\n",
       " 'Perturbative Likelihoods for Large-Scale Structure of the Universe',\n",
       " 'Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?',\n",
       " 'Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons',\n",
       " 'Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence',\n",
       " \"To Trust Or Not To Trust Your Vision-Language Model's Prediction\",\n",
       " 'Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need',\n",
       " 'DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP',\n",
       " 'MAGREF: Masked Guidance for Any-Reference Video Generation',\n",
       " 'LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization',\n",
       " \"How Animals Dance (When You're Not Looking)\",\n",
       " 'ATLAS: Learning to Optimally Memorize the Context at Test Time',\n",
       " 'ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS',\n",
       " 'Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side',\n",
       " 'EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast',\n",
       " 'Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time',\n",
       " 'Group Convolutional Neural Network Ground State of the Quantum Dimer Model',\n",
       " 'PixelThink: Towards Efficient Chain-of-Pixel Reasoning',\n",
       " 'FMG-Det: Foundation Model Guided Robust Object Detection',\n",
       " 'MuLoCo: Muon is a practical inner optimizer for DiLoCo',\n",
       " 'SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA',\n",
       " 'ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering',\n",
       " 'Label-Guided In-Context Learning for Named Entity Recognition',\n",
       " 'DiffER: Categorical Diffusion for Chemical Retrosynthesis',\n",
       " 'TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning',\n",
       " 'Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?',\n",
       " 'AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views',\n",
       " \"Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models\",\n",
       " 'SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods',\n",
       " 'SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models',\n",
       " 'Neutron Stars in Causal Scalar-Tensor Theories',\n",
       " 'Hub Detection in Gaussian Graphical Models',\n",
       " 'Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats',\n",
       " 'Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better',\n",
       " \"Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability\",\n",
       " 'CLDTracker: A Comprehensive Language Description for Visual Tracking',\n",
       " '(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d818a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Summary: {paper['summary']}\\n\"\n",
    "            f\"Published: {paper['published']}\\n\"\n",
    "            f\"Journal Reference: {paper['journal_ref']}\\n\"\n",
    "            f\"DOI: {paper['doi']}\\n\"\n",
    "            f\"Primary Category: {paper['primary_category']}\\n\"\n",
    "            f\"Categories: {', '.join(paper['categories'])}\\n\"\n",
    "            f\"PDF URL: {paper['pdf_url']}\\n\"\n",
    "            f\"arXiv URL: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "        documents.append(Document(text=content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75d671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48377ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='ba8b21c0-4175-4c37-906b-db64d1e900ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models\\nAuthors: Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem\\nSummary: Image-text models excel at image-level tasks but struggle with detailed\\nvisual understanding. While these models provide strong visual-language\\nalignment, segmentation models like SAM2 offer precise spatial boundaries for\\nobjects. To this end, we propose TextRegion, a simple, effective, and\\ntraining-free framework that combines the strengths of image-text models and\\nSAM2 to generate powerful text-aligned region tokens. These tokens enable\\ndetailed visual understanding while preserving open-vocabulary capabilities.\\nThey can be directly applied to various downstream tasks, including open-world\\nsemantic segmentation, referring expression comprehension, and grounding. We\\nconduct extensive evaluations and consistently achieve superior or competitive\\nperformance compared to state-of-the-art training-free methods. Additionally,\\nour framework is compatible with many image-text models, making it highly\\npractical and easily extensible as stronger models emerge. Code is available\\nat: https://github.com/avaxiao/TextRegion.\\nPublished: 2025-05-29 17:59:59+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23769v1\\narXiv URL: http://arxiv.org/abs/2505.23769v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='560f3b80-5fa7-4704-8c6c-a7b0087917b3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought\\nAuthors: Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu\\nSummary: Recent advances in multimodal large language models (MLLMs) have demonstrated\\nremarkable capabilities in vision-language tasks, yet they often struggle with\\nvision-centric scenarios where precise visual focus is needed for accurate\\nreasoning. In this paper, we introduce Argus to address these limitations with\\na new visual attention grounding mechanism. Our approach employs object-centric\\ngrounding as visual chain-of-thought signals, enabling more effective\\ngoal-conditioned visual attention during multimodal reasoning tasks.\\nEvaluations on diverse benchmarks demonstrate that Argus excels in both\\nmultimodal reasoning tasks and referring object grounding tasks. Extensive\\nanalysis further validates various design choices of Argus, and reveals the\\neffectiveness of explicit language-guided visual region-of-interest engagement\\nin MLLMs, highlighting the importance of advancing multimodal intelligence from\\na visual-centric perspective. Project page: https://yunzeman.github.io/argus/\\nPublished: 2025-05-29 17:59:56+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23766v1\\narXiv URL: http://arxiv.org/abs/2505.23766v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='894fb2de-7d83-4295-b529-9d6e309be67f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: From Chat Logs to Collective Insights: Aggregative Question Answering\\nAuthors: Wentao Zhang, Woojeong Kim, Yuntian Deng\\nSummary: Conversational agents powered by large language models (LLMs) are rapidly\\nbecoming integral to our daily interactions, generating unprecedented amounts\\nof conversational data. Such datasets offer a powerful lens into societal\\ninterests, trending topics, and collective concerns. Yet, existing approaches\\ntypically treat these interactions as independent and miss critical insights\\nthat could emerge from aggregating and reasoning across large-scale\\nconversation logs. In this paper, we introduce Aggregative Question Answering,\\na novel task requiring models to reason explicitly over thousands of\\nuser-chatbot interactions to answer aggregative queries, such as identifying\\nemerging concerns among specific demographics. To enable research in this\\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\\nquestions derived from 182,330 real-world chatbot conversations. Experiments\\nshow that existing methods either struggle to reason effectively or incur\\nprohibitive computational costs, underscoring the need for new approaches\\ncapable of extracting collective insights from large-scale conversational data.\\nPublished: 2025-05-29 17:59:55+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI, cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23765v1\\narXiv URL: http://arxiv.org/abs/2505.23765v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c67ee979-03ab-4df1-96cc-b8d51ed6aa87', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence\\nAuthors: Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang\\nSummary: Spatial intelligence is essential for multimodal large language models\\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\\nprobe only single-image relations and thus fail to assess the multi-image\\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\\nunambiguous multiple-choice questions from over 120,000 images, each paired\\nwith carefully designed distractors and a step-by-step reasoning process. We\\nconduct extensive experiments and thoroughly evaluate 34 open-source and\\nproprietary MLLMs, observing a wide gap: the strongest open-source model\\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\\nand the substantial headroom for future research. Leveraging the annotated\\nreasoning processes, we also provide an automated error analysis pipeline that\\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\\nadvancing multi-image spatial intelligence. Project page:\\nhttps://runsenxu.com/projects/MMSI_Bench .\\nPublished: 2025-05-29 17:59:52+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.CL\\nPDF URL: http://arxiv.org/pdf/2505.23764v1\\narXiv URL: http://arxiv.org/abs/2505.23764v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='acc1dc7c-d522-4594-8008-15057cae3c58', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ZeroGUI: Automating Online GUI Learning at Zero Human Cost\\nAuthors: Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai\\nSummary: The rapid advancement of large Vision-Language Models (VLMs) has propelled\\nthe development of pure-vision-based GUI Agents, capable of perceiving and\\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\\ninstructions. However, existing approaches usually adopt an offline learning\\nframework, which faces two core limitations: (1) heavy reliance on high-quality\\nmanual annotations for element grounding and action supervision, and (2)\\nlimited adaptability to dynamic and interactive environments. To address these\\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\\nintegrates (i) VLM-based automatic task generation to produce diverse training\\ngoals from the current environment state, (ii) VLM-based automatic reward\\nestimation to assess task success without hand-crafted evaluation functions,\\nand (iii) two-stage online reinforcement learning to continuously interact with\\nand learn from GUI environments. Experiments on two advanced GUI Agents\\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\\nacross OSWorld and AndroidLab environments. The code is available at\\nhttps://github.com/OpenGVLab/ZeroGUI.\\nPublished: 2025-05-29 17:59:51+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.AI\\nCategories: cs.AI, cs.CL, cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23762v1\\narXiv URL: http://arxiv.org/abs/2505.23762v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3b64b284-a5c6-4bac-bbc7-e9e1b5fa4a4b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch\\nAuthors: Aneeshan Sain, Subhajit Maity, Pinaki Nath Chowdhury, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song\\nSummary: As sketch research has collectively matured over time, its adaptation for\\nat-mass commercialisation emerges on the immediate horizon. Despite an already\\nmature research endeavour for photos, there is no research on the efficient\\ninference specifically designed for sketch data. In this paper, we first\\ndemonstrate existing state-of-the-art efficient light-weight models designed\\nfor photos do not work on sketches. We then propose two sketch-specific\\ncomponents which work in a plug-n-play manner on any photo efficient network to\\nadapt them to work on sketch data. We specifically chose fine-grained\\nsketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised\\nsketch problem with immediate commercial value. Technically speaking, we first\\npropose a cross-modal knowledge distillation network to transfer existing photo\\nefficient networks to be compatible with sketch, which brings down number of\\nFLOPs and model parameters by 97.96% percent and 84.89% respectively. We then\\nexploit the abstract trait of sketch to introduce a RL-based canvas selector\\nthat dynamically adjusts to the abstraction level which further cuts down\\nnumber of FLOPs by two thirds. The end result is an overall reduction of 99.37%\\nof FLOPs (from 40.18G to 0.254G) when compared with a full network, while\\nretaining the accuracy (33.03% vs 32.77%) -- finally making an efficient\\nnetwork for the sparse sketch data that exhibit even fewer FLOPs than the best\\nphoto counterpart.\\nPublished: 2025-05-29 17:59:51+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23763v1\\narXiv URL: http://arxiv.org/abs/2505.23763v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae1a122d-5b64-4f93-b32f-b78cd7ef89cf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Differential Information: An Information-Theoretic Perspective on Preference Optimization\\nAuthors: Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo\\nSummary: Direct Preference Optimization (DPO) has become a standard technique for\\naligning language models with human preferences in a supervised manner. Despite\\nits empirical success, the theoretical justification behind its log-ratio\\nreward parameterization remains incomplete. In this work, we address this gap\\nby utilizing the Differential Information Distribution (DID): a distribution\\nover token sequences that captures the information gained during policy\\nupdates. First, we show that when preference labels encode the differential\\ninformation required to transform a reference policy into a target policy, the\\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\\ntarget policy via preference optimization. This result naturally yields a\\nclosed-form expression for the optimal sampling distribution over rejected\\nresponses. Second, we find that the condition for preferences to encode\\ndifferential information is fundamentally linked to an implicit assumption\\nregarding log-margin ordered policies-an inductive bias widely used in\\npreference optimization yet previously unrecognized. Finally, by analyzing the\\nentropy of the DID, we characterize how learning low-entropy differential\\ninformation reinforces the policy distribution, while high-entropy differential\\ninformation induces a smoothing effect, which explains the log-likelihood\\ndisplacement phenomenon. We validate our theoretical findings in synthetic\\nexperiments and extend them to real-world instruction-following datasets. Our\\nresults suggest that learning high-entropy differential information is crucial\\nfor general instruction-following, while learning low-entropy differential\\ninformation benefits knowledge-intensive question answering. Overall, our work\\npresents a unifying perspective on the DPO objective, the structure of\\npreference data, and resulting policy behaviors through the lens of\\ndifferential information.\\nPublished: 2025-05-29 17:59:50+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.AI, cs.CL\\nPDF URL: http://arxiv.org/pdf/2505.23761v1\\narXiv URL: http://arxiv.org/abs/2505.23761v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fe3210f-d726-47f9-8701-61dc9995a322', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Model Immunization from a Condition Number Perspective\\nAuthors: Amber Yijia Zheng, Cedar Site Bai, Brian Bullins, Raymond A. Yeh\\nSummary: Model immunization aims to pre-train models that are difficult to fine-tune\\non harmful tasks while retaining their utility on other non-harmful tasks.\\nThough prior work has shown empirical evidence for immunizing text-to-image\\nmodels, the key understanding of when immunization is possible and a precise\\ndefinition of an immunized model remain unclear. In this work, we propose a\\nframework, based on the condition number of a Hessian matrix, to analyze model\\nimmunization for linear models. Building on this framework, we design an\\nalgorithm with regularization terms to control the resulting condition numbers\\nafter pre-training. Empirical results on linear models and non-linear deep-nets\\ndemonstrate the effectiveness of the proposed algorithm on model immunization.\\nThe code is available at\\nhttps://github.com/amberyzheng/model-immunization-cond-num.\\nPublished: 2025-05-29 17:59:48+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23760v1\\narXiv URL: http://arxiv.org/abs/2505.23760v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7c28f641-cd9b-421e-9d0b-fe59cab282f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Puzzled by Puzzles: When Vision-Language Models Can\\'t Take a Hint\\nAuthors: Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan\\nSummary: Rebus puzzles, visual riddles that encode language through imagery, spatial\\narrangement, and symbolic substitution, pose a unique challenge to current\\nvision-language models (VLMs). Unlike traditional image captioning or question\\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\\nrebus puzzles by constructing a hand-generated and annotated benchmark of\\ndiverse English-language rebus puzzles, ranging from simple pictographic\\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\\nsurprising capabilities in decoding simple visual clues, they struggle\\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\\nunderstanding visual metaphors.\\nPublished: 2025-05-29 17:59:47+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI, cs.CV, cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23759v1\\narXiv URL: http://arxiv.org/abs/2505.23759v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e51b5620-743b-4736-830b-1dbf2dea5ffb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models\\nAuthors: Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao\\nSummary: Vision-Language-Action (VLA) models for autonomous driving show promise but\\nfalter in unstructured corner case scenarios, largely due to a scarcity of\\ntargeted benchmarks. To address this, we introduce Impromptu VLA. Our core\\ncontribution is the Impromptu VLA Dataset: over 80,000 meticulously curated\\nvideo clips, distilled from over 2M source clips sourced from 8 open-source\\nlarge-scale datasets. This dataset is built upon our novel taxonomy of four\\nchallenging unstructured categories and features rich, planning-oriented\\nquestion-answering annotations and action trajectories. Crucially, experiments\\ndemonstrate that VLAs trained with our dataset achieve substantial performance\\ngains on established benchmarks--improving closed-loop NeuroNCAP scores and\\ncollision rates, and reaching near state-of-the-art L2 accuracy in open-loop\\nnuScenes trajectory prediction. Furthermore, our Q&A suite serves as an\\neffective diagnostic, revealing clear VLM improvements in perception,\\nprediction, and planning. Our code, data and models are available at\\nhttps://github.com/ahydchh/Impromptu-VLA.\\nPublished: 2025-05-29 17:59:46+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23757v1\\narXiv URL: http://arxiv.org/abs/2505.23757v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1747ec9-cc12-4898-b8c0-a878e17f3a37', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers\\nAuthors: Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag\\nSummary: We introduce LoRAShop, the first framework for multi-concept image editing\\nwith LoRA models. LoRAShop builds on a key observation about the feature\\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\\ntransformer features activate spatially coherent regions early in the denoising\\nprocess. We harness this observation to derive a disentangled latent mask for\\neach concept in a prior forward pass and blend the corresponding LoRA weights\\nonly within regions bounding the concepts to be personalized. The resulting\\nedits seamlessly integrate multiple subjects or styles into the original scene\\nwhile preserving global context, lighting, and fine details. Our experiments\\ndemonstrate that LoRAShop delivers better identity preservation compared to\\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\\nopens new avenues for compositional visual storytelling and rapid creative\\niteration.\\nPublished: 2025-05-29 17:59:46+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23758v1\\narXiv URL: http://arxiv.org/abs/2505.23758v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2ca8d23-e877-457c-b2a8-e70d47b0a153', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning\\nAuthors: Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu\\nSummary: Theorem proving serves as a major testbed for evaluating complex reasoning\\nabilities in large language models (LLMs). However, traditional automated\\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\\npoorly align with LLMs' strength derived from informal, natural language\\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\\ncomprehensive informal theorem-proving framework exploiting natural language to\\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\\nand proofs spanning diverse mathematical domains, rigorously annotated for\\ncorrectness, difficulty, and topic categories, accompanied by systematically\\nconstructed verifiable theorem variants. We devise a novel reinforcement\\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\\nleveraging the verified theorem variants to incentivize robust mathematical\\ninference. Additionally, we propose comprehensive outcome and process\\nevaluation metrics examining proof correctness and the quality of reasoning\\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\\nimproves LLM theorem-proving performance compared to existing datasets and\\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\\nreasoning quality. Our findings highlight DeepTheorem's potential to\\nfundamentally advance automated informal theorem proving and mathematical\\nexploration.\\nPublished: 2025-05-29 17:59:39+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23754v1\\narXiv URL: http://arxiv.org/abs/2505.23754v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0af12532-6506-4387-b3d7-a2517d717268', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks\\nAuthors: Akashah Shabbir, Muhammad Akhtar Munir, Akshay Dudhane, Muhammad Umer Sheikh, Muhammad Haris Khan, Paolo Fraccaro, Juan Bernabe Moreno, Fahad Shahbaz Khan, Salman Khan\\nSummary: Recent progress in large language models (LLMs) has enabled tool-augmented\\nagents capable of solving complex real-world tasks through step-by-step\\nreasoning. However, existing evaluations often focus on general-purpose or\\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\\ntasks via structured tool use and multi-step planning. Inspired by\\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\\nwide range of real-world applications such as urban planning, disaster\\nassessment and change analysis, environmental monitoring, transportation\\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\\nanalysis. Each query is grounded in satellite or aerial imagery and requires\\nagents to reason through a diverse toolset. We implement a ReAct-style\\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\\nexecution metrics and final answer correctness. Our analysis reveals notable\\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\\navailable\\nPublished: 2025-05-29 17:59:38+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23752v1\\narXiv URL: http://arxiv.org/abs/2505.23752v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c07f2931-4a04-48a4-9b0c-bb691efa12bd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: REOrdering Patches Improves Vision Models\\nAuthors: Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta\\nSummary: Sequence models such as transformers require inputs to be represented as\\none-dimensional sequences. In vision, this typically involves flattening images\\nusing a fixed row-major (raster-scan) order. While full self-attention is\\npermutation-equivariant, modern long-sequence transformers increasingly rely on\\narchitectural approximations that break this invariance and introduce\\nsensitivity to patch ordering. We show that patch order significantly affects\\nmodel performance in such settings, with simple alternatives like column-major\\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\\npropose REOrder, a two-stage framework for discovering task-optimal patch\\norderings. First, we derive an information-theoretic prior by evaluating the\\ncompressibility of various patch sequences. Then, we learn a policy over\\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\\napproach enables efficient learning in a combinatorial permutation space.\\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\\n3.01% and Functional Map of the World by 13.35%.\\nPublished: 2025-05-29 17:59:30+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.AI, cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23751v1\\narXiv URL: http://arxiv.org/abs/2505.23751v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6fbdfa23-1653-4229-b7cd-de9e494623bd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Perturbative Likelihoods for Large-Scale Structure of the Universe\\nAuthors: Rodrigo Voivodic\\nSummary: This work presents a formalism for deriving likelihoods of the cosmological\\ndensity field directly from first principles within Perturbation Theory (PT).\\nBy assuming a perturbative expansion around the Gaussian initial density field\\nand additional stochastic components, we analytically compute two forms of the\\nlikelihood. Full marginalization over all underlying fields yields the\\nlikelihood of the observed density field, expressed in terms of its summary\\nstatistics (such as the power spectrum and bispectrum), which are naturally\\ngiven by the formalism, and conditioned on model parameters. Marginalizing only\\nover the stochastic fields results in the field-level likelihood. A key\\nstrength of this method is its ability to automatically specify the precise\\ncombinations of initial field covariances and PT expansion kernels required at\\neach perturbative order (e.g., tree-level power spectrum and bispectrum, and\\nthe 1-loop power spectrum). This guarantees that the resulting likelihoods are\\nfully consistent with PT at the chosen order of accuracy, avoiding ad-hoc\\nchoices in constructing the statistical model.\\nPublished: 2025-05-29 17:59:25+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: astro-ph.CO\\nCategories: astro-ph.CO\\nPDF URL: http://arxiv.org/pdf/2505.23750v1\\narXiv URL: http://arxiv.org/abs/2505.23750v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75585f76-3451-48c7-a29f-5e9a29bd01e1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?\\nAuthors: Paul Gölz, Nika Haghtalab, Kunhe Yang\\nSummary: After pre-training, large language models are aligned with human preferences\\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\\npreference model, despite being deployed in settings where users have diverse\\npreferences. As a result, it is not even clear that these alignment methods\\nproduce models that satisfy users on average -- a minimal requirement for\\npluralistic alignment. Drawing on social choice theory and modeling users'\\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\\nalignment method's distortion: the worst-case ratio between the optimal\\nachievable average utility, and the average utility of the learned policy.\\n  The notion of distortion helps draw sharp distinctions between alignment\\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\\ndistortion of $(\\\\frac{1}{2} + o(1)) \\\\cdot \\\\beta$ (for the BT temperature\\n$\\\\beta$), robustly across utility distributions, distributions of comparison\\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\\nby contrast, suffer $\\\\geq (1 - o(1)) \\\\cdot \\\\beta$ distortion already without a\\nKL constraint, and $e^{\\\\Omega(\\\\beta)}$ or even unbounded distortion in the full\\nsetting, depending on how comparison pairs are sampled.\\nPublished: 2025-05-29 17:59:20+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.GT\\nPDF URL: http://arxiv.org/pdf/2505.23749v1\\narXiv URL: http://arxiv.org/abs/2505.23749v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d493049d-dc61-4a87-bd1e-125b7476aba2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons\\nAuthors: Hugo Henry, Kelly Cohen\\nSummary: This study investigates the application of Genetic Fuzzy Systems (GFS) to\\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\\nsignificant implications for aerospace, automotive and drone applications.\\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\\nregression strategies are explored and compared. The paper evaluates a brute\\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\\nviability of clustering assisted fuzzy inference as an effective regression\\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\\nCascading systems, Clustering and AI.\\nPublished: 2025-05-29 17:59:04+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.AI\\nCategories: cs.AI, cs.NE\\nPDF URL: http://arxiv.org/pdf/2505.23746v1\\narXiv URL: http://arxiv.org/abs/2505.23746v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fc43ba29-430b-4711-9697-1e37bdb9d534', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence\\nAuthors: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan\\nSummary: Recent advancements in Multimodal Large Language Models (MLLMs) have\\nsignificantly enhanced performance on 2D visual tasks. However, improving their\\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\\nutility in scenarios with only 2D inputs, such as images or videos. In this\\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\\nrely on CLIP-based visual encoders optimized for semantic understanding, our\\nkey insight is to unleash the strong structure prior from the feed-forward\\nvisual geometry foundation model. Specifically, we propose a dual-encoder\\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\\na spatial encoder-initialized from the backbone of the visual geometry model-to\\nextract 3D structure features. A connector then integrates both features into\\nunified visual tokens for enhanced spatial understanding. Furthermore, we\\npropose a space-aware frame sampling strategy at inference time, which selects\\nthe spatially informative frames of a video sequence, ensuring that even under\\nlimited token length, the model focuses on frames critical for spatial\\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\\ndataset and train the model on it using supervised fine-tuning and GRPO.\\nExtensive experiments on various real-world datasets demonstrate that our\\nspatial-MLLM achieves state-of-the-art performance in a wide range of\\nvisual-based spatial understanding and reasoning tasks. Project page:\\nhttps://diankun-wu.github.io/Spatial-MLLM/.\\nPublished: 2025-05-29 17:59:04+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI, cs.LG, I.2.6; I.2\\nPDF URL: http://arxiv.org/pdf/2505.23747v1\\narXiv URL: http://arxiv.org/abs/2505.23747v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c037e15e-6e89-4f52-ab2a-bab0dccd507e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: To Trust Or Not To Trust Your Vision-Language Model's Prediction\\nAuthors: Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink\\nSummary: Vision-Language Models (VLMs) have demonstrated strong capabilities in\\naligning visual and textual modalities, enabling a wide range of applications\\nin multimodal understanding and generation. While they excel in zero-shot and\\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\\noften yielding confident yet incorrect predictions. This limitation poses a\\nsignificant risk in safety-critical domains, where erroneous predictions can\\nlead to severe consequences. In this work, we introduce TrustVLM, a\\ntraining-free framework designed to address the critical challenge of\\nestimating when VLM's predictions can be trusted. Motivated by the observed\\nmodality gap in VLMs and the insight that certain concepts are more distinctly\\nrepresented in the image embedding space, we propose a novel confidence-scoring\\nfunction that leverages this space to improve misclassification detection. We\\nrigorously evaluate our approach across 17 diverse datasets, employing 4\\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\\ncompared to existing baselines. By improving the reliability of the model\\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\\nVLMs in real-world applications. The code will be available at\\nhttps://github.com/EPFL-IMOS/TrustVLM.\\nPublished: 2025-05-29 17:59:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI, cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23745v1\\narXiv URL: http://arxiv.org/abs/2505.23745v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d8bb857-acf4-4d03-9f8b-544140dc18a5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need\\nAuthors: Qiang Wang, Xiang Song, Yuhang He, Jizhou Han, Chenhao Ding, Xinyuan Gao, Yihong Gong\\nSummary: Deep neural networks (DNNs) often underperform in real-world, dynamic\\nsettings where data distributions change over time. Domain Incremental Learning\\n(DIL) offers a solution by enabling continual model adaptation, with\\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\\nselection accuracy, especially as the number of domains and corresponding\\nclasses grows. To address this, we propose SOYO, a lightweight framework that\\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\\n(MDFN) enhances domain feature extraction. Our framework supports multiple\\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\\nsuch as image classification, object detection, and speech enhancement.\\nExperimental results on six benchmarks demonstrate SOYO's consistent\\nsuperiority over existing baselines, showcasing its robustness and adaptability\\nin complex, evolving environments. The codes will be released in\\nhttps://github.com/qwangcv/SOYO.\\nPublished: 2025-05-29 17:58:57+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23744v1\\narXiv URL: http://arxiv.org/abs/2505.23744v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b534c569-0268-410a-b707-0349248ca036', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP\\nAuthors: Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen\\nSummary: High-quality photography in extreme low-light conditions is challenging but\\nimpactful for digital cameras. With advanced computing hardware, traditional\\ncamera image signal processor (ISP) algorithms are gradually being replaced by\\nefficient deep networks that enhance noisy raw images more intelligently.\\nHowever, existing regression-based models often minimize pixel errors and\\nresult in oversmoothing of low-light photos or deep shadows. Recent work has\\nattempted to address this limitation by training a diffusion model from\\nscratch, yet those models still struggle to recover sharp image details and\\naccurate colors. We introduce a novel framework to enhance low-light raw images\\nby retasking pre-trained generative diffusion models with the camera ISP.\\nExtensive experiments demonstrate that our method outperforms the\\nstate-of-the-art in perceptual quality across three challenging low-light raw\\nimage benchmarks.\\nPublished: 2025-05-29 17:58:48+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, eess.IV\\nPDF URL: http://arxiv.org/pdf/2505.23743v1\\narXiv URL: http://arxiv.org/abs/2505.23743v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='95f415b2-9fd6-49fd-9608-cdc0629e583a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: MAGREF: Masked Guidance for Any-Reference Video Generation\\nAuthors: Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma\\nSummary: Video generation has made substantial strides with the emergence of deep\\ngenerative models, especially diffusion-based approaches. However, video\\ngeneration based on multiple reference subjects still faces significant\\nchallenges in maintaining multi-subject consistency and ensuring high\\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\\nany-reference video generation that introduces masked guidance to enable\\ncoherent multi-subject video synthesis conditioned on diverse reference images\\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\\nmasking mechanism that enables a single model to flexibly handle various\\nsubject inference, including humans, objects, and backgrounds, without\\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\\nthat operates on the channel dimension to better preserve appearance features.\\nOur model delivers state-of-the-art video generation quality, generalizing from\\nsingle-subject training to complex multi-subject scenarios with coherent\\nsynthesis and precise control over individual subjects, outperforming existing\\nopen-source and commercial baselines. To facilitate evaluation, we also\\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\\ndemonstrate the effectiveness of our approach, paving the way for scalable,\\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\\ncan be found at: https://github.com/MAGREF-Video/MAGREF\\nPublished: 2025-05-29 17:58:15+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23742v1\\narXiv URL: http://arxiv.org/abs/2505.23742v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d0224f91-2b07-4ad6-a346-ca48c318745e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization\\nAuthors: Ronghuan Wu, Wanchao Su, Jing Liao\\nSummary: Image vectorization is a powerful technique that converts raster images into\\nvector graphics, enabling enhanced flexibility and interactivity. However,\\npopular image vectorization tools struggle with occluded regions, producing\\nincomplete or fragmented shapes that hinder editability. While recent\\nadvancements have explored rule-based and data-driven layer-wise image\\nvectorization, these methods face limitations in vectorization quality and\\nflexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image\\nvectorization approach that addresses these challenges through a progressive\\nsimplification paradigm. The key to LayerPeeler's success lies in its\\nautoregressive peeling strategy: by identifying and removing the topmost\\nnon-occluded layers while recovering underlying content, we generate vector\\ngraphics with complete paths and coherent layer structures. Our method\\nleverages vision-language models to construct a layer graph that captures\\nocclusion relationships among elements, enabling precise detection and\\ndescription for non-occluded layers. These descriptive captions are used as\\nediting instructions for a finetuned image diffusion model to remove the\\nidentified layers. To ensure accurate removal, we employ localized attention\\ncontrol that precisely guides the model to target regions while faithfully\\npreserving the surrounding content. To support this, we contribute a\\nlarge-scale dataset specifically designed for layer peeling tasks. Extensive\\nquantitative and qualitative experiments demonstrate that LayerPeeler\\nsignificantly outperforms existing techniques, producing vectorization results\\nwith superior path semantics, geometric regularity, and visual fidelity.\\nPublished: 2025-05-29 17:58:03+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.GR\\nPDF URL: http://arxiv.org/pdf/2505.23740v1\\narXiv URL: http://arxiv.org/abs/2505.23740v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c4140ae-b2b7-4d56-9406-c110df410b8d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: How Animals Dance (When You're Not Looking)\\nAuthors: Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz\\nSummary: We present a keyframe-based framework for generating music-synchronized,\\nchoreography aware animal dance videos. Starting from a few keyframes\\nrepresenting distinct animal poses -- generated via text-to-image prompting or\\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\\nthe optimal keyframe structure that satisfies a specified choreography pattern\\nof beats, which can be automatically estimated from a reference dance video. We\\nalso introduce an approach for mirrored pose image generation, essential for\\ncapturing symmetry in dance. In-between frames are synthesized using an video\\ndiffusion model. With as few as six input keyframes, our method can produce up\\nto 30 second dance videos across a wide range of animals and music tracks.\\nPublished: 2025-05-29 17:58:02+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.GR\\nPDF URL: http://arxiv.org/pdf/2505.23738v1\\narXiv URL: http://arxiv.org/abs/2505.23738v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fb605479-cd3b-44b4-8897-f0f94ee79932', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ATLAS: Learning to Optimally Memorize the Context at Test Time\\nAuthors: Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni\\nSummary: Transformers have been established as the most popular backbones in sequence\\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\\nthe ability to learn at scale. Their quadratic memory and time complexity,\\nhowever, bound their applicability in longer sequences and so has motivated\\nresearchers to explore effective alternative architectures such as modern\\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\\ntheir recent success in diverse downstream tasks, they struggle in tasks that\\nrequires long context understanding and extrapolation to longer sequences. We\\nobserve that these shortcomings come from three disjoint aspects in their\\ndesign: (1) limited memory capacity that is bounded by the architecture of\\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\\noptimizing the memory only with respect to the last input; and (3) less\\nexpressive management of their fixed-size memory. To enhance all these three\\naspects, we present ATLAS, a long-term memory module with high capacity that\\nlearns to memorize the context by optimizing the memory based on the current\\nand past tokens, overcoming the online nature of long-term memory models.\\nBuilding on this insight, we present a new family of Transformer-like\\narchitectures, called DeepTransformers, that are strict generalizations of the\\noriginal Transformer architecture. Our experimental results on language\\nmodeling, common-sense reasoning, recall-intensive, and long-context\\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\\nand recent linear recurrent models. ATLAS further improves the long context\\nperformance of Titans, achieving +80\\\\% accuracy in 10M context length of\\nBABILong benchmark.\\nPublished: 2025-05-29 17:57:16+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23735v1\\narXiv URL: http://arxiv.org/abs/2505.23735v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='925c340b-0891-4f03-923b-f92e70c2222a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS\\nAuthors: Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang\\nSummary: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\\npromising solution for novel view synthesis, enabling one-pass inference\\nwithout the need for per-scene 3DGS optimization. However, their scalability is\\nfundamentally constrained by the limited capacity of their encoders, leading to\\ndegraded performance or excessive memory consumption as the number of input\\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\\nlightweight architecture-agnostic module that enables efficient compression of\\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\\ninformation while discarding redundancy. Concretely, ZPressor enables existing\\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\\nan 80GB GPU, by partitioning the views into anchor and support sets and using\\ncross attention to compress the information from the support views into anchor\\nviews, forming the compressed latent state $Z$. We show that integrating\\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\\nimproves performance under moderate input views and enhances robustness under\\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\\nThe video results, code and trained models are available on our project page:\\nhttps://lhmd.top/zpressor.\\nPublished: 2025-05-29 17:57:04+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23734v1\\narXiv URL: http://arxiv.org/abs/2505.23734v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d68e22c8-6057-4e0d-ba2d-b0503a42e193', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side\\nAuthors: Truong, Luu, Binny M. Samuel\\nSummary: In recent years, the rapid advancement and democratization of generative AI\\nmodels have sparked significant debate over safety, ethical risks, and dual-use\\nconcerns, particularly in the context of cybersecurity. While anecdotally\\nknown, this paper provides empirical evidence regarding generative AI's\\nassociation with malicious internet-related activities and cybercrime by\\nexamining the phenomenon through psychological frameworks of technological\\namplification and affordance theory. Using a quasi-experimental design with\\ninterrupted time series analysis, we analyze two datasets, one general and one\\ncryptocurrency-focused, to empirically assess generative AI's role in\\ncybercrime. The findings contribute to ongoing discussions about AI governance\\nby balancing control and fostering innovation, underscoring the need for\\nstrategies to guide policymakers, inform AI developers and cybersecurity\\nprofessionals, and educate the public to maximize AI's benefits while\\nmitigating its risks.\\nPublished: 2025-05-29 17:57:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CY\\nCategories: cs.CY, cs.AI, cs.HC\\nPDF URL: http://arxiv.org/pdf/2505.23733v1\\narXiv URL: http://arxiv.org/abs/2505.23733v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='26492c8a-ad3d-491a-b05c-8ec979c8c4a8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast\\nAuthors: Shreeram Suresh Chandra, Lucas Goncalves, Junchen Lu, Carlos Busso, Berrak Sisman\\nSummary: Current emotion-based contrastive language-audio pretraining (CLAP) methods\\ntypically learn by na\\\\\"ively aligning audio samples with corresponding text\\nprompts. Consequently, this approach fails to capture the ordinal nature of\\nemotions, hindering inter-emotion understanding and often resulting in a wide\\nmodality gap between the audio and text embeddings due to insufficient\\nalignment. To handle these drawbacks, we introduce EmotionRankCLAP, a\\nsupervised contrastive learning approach that uses dimensional attributes of\\nemotional speech and natural language prompts to jointly capture fine-grained\\nemotion variations and improve cross-modal alignment. Our approach utilizes a\\nRank-N-Contrast objective to learn ordered relationships by contrasting samples\\nbased on their rankings in the valence-arousal space. EmotionRankCLAP\\noutperforms existing emotion-CLAP methods in modeling emotion ordinality across\\nmodalities, measured via a cross-modal retrieval task.\\nPublished: 2025-05-29 17:56:55+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23732v1\\narXiv URL: http://arxiv.org/abs/2505.23732v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='29e9a016-e630-459b-8c51-edd50a375018', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time\\nAuthors: Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi\\nSummary: Aligning large language models with humans is challenging due to the\\ninherently multifaceted nature of preference feedback. While existing\\napproaches typically frame this as a multi-objective optimization problem, they\\noften overlook how humans actually make decisions. Research on bounded\\nrationality suggests that human decision making follows satisficing\\nstrategies-optimizing primary objectives while ensuring others meet acceptable\\nthresholds. To bridge this gap and operationalize the notion of satisficing\\nalignment, we propose SITAlign: an inference time framework that addresses the\\nmultifaceted nature of alignment by maximizing a primary objective while\\nsatisfying threshold-based constraints on secondary criteria. We provide\\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\\ninference alignment approach. We empirically validate SITAlign's performance\\nthrough extensive experimentation on multiple benchmarks. For instance, on the\\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\\nwin-tie rate for helpfulness reward while adhering to the threshold on\\nharmlessness.\\nPublished: 2025-05-29 17:56:05+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23729v1\\narXiv URL: http://arxiv.org/abs/2505.23729v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ec4d0fc-d919-4808-a0c4-ba38964c314a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Group Convolutional Neural Network Ground State of the Quantum Dimer Model\\nAuthors: Ojasvi Sharma, Sandipan Manna, Prashant Shekhar Rao, G J Sreejith\\nSummary: We estimate the ground state of the square lattice Quantum Dimer Model in a\\n$\\\\rm{p4m}$-symmetric Group Convolutional Neural Network (GCNN) representation\\nand show that results in agreement with exact diagonalization (ED) and quantum\\nMonte Carlo (QMC) can be obtained with a $\\\\mathcal{L}=2$ layer network. In\\nsystems of linear size $L=8$ with Hilbert space dimension $3.1\\\\times 10^8$,\\nGCNN shows fidelity as high as $0.99999$ with ED. For $12\\\\leq L\\\\leq 32$, we\\nfind excellent agreement with QMC estimates of energy, order parameters and\\ncorrelation functions. The network is optimized by minimizing the energy\\nestimated from a Metropolis algorithm assisted by a directed loop sampler. We\\nanalyze the quantum geometric tensor at the minima for $\\\\mathcal{L}=1,2$ and\\n$3$ and show that the empirical quantum dimension saturates with increasing\\nnetwork complexity due to Metropolis sampling constraints.\\nPublished: 2025-05-29 17:56:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cond-mat.dis-nn\\nCategories: cond-mat.dis-nn, cond-mat.stat-mech, cond-mat.str-el\\nPDF URL: http://arxiv.org/pdf/2505.23728v1\\narXiv URL: http://arxiv.org/abs/2505.23728v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5e13d3c3-b50d-404b-aa9f-52e5350b4d44', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: PixelThink: Towards Efficient Chain-of-Pixel Reasoning\\nAuthors: Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, Xinchao Wang\\nSummary: Existing reasoning segmentation approaches typically fine-tune multimodal\\nlarge language models (MLLMs) using image-text pairs and corresponding mask\\nlabels. However, they exhibit limited generalization to out-of-distribution\\nscenarios without an explicit reasoning process. Although recent efforts\\nleverage reinforcement learning through group-relative policy optimization\\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\\nproducing uniformly verbose reasoning chains irrespective of task complexity.\\nThis results in elevated computational costs and limited control over reasoning\\nquality. To address this problem, we propose PixelThink, a simple yet effective\\nscheme that integrates externally estimated task difficulty and internally\\nmeasured model uncertainty to regulate reasoning generation within a\\nreinforcement learning paradigm. The model learns to compress reasoning length\\nin accordance with scene complexity and predictive confidence. To support\\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\\nwith annotated reasoning references and difficulty scores, along with a suite\\nof metrics designed to assess segmentation accuracy, reasoning quality, and\\nefficiency jointly. Experimental results demonstrate that the proposed approach\\nimproves both reasoning efficiency and overall segmentation performance. Our\\nwork contributes novel perspectives towards efficient and interpretable\\nmultimodal understanding. The code and model will be publicly available.\\nPublished: 2025-05-29 17:55:49+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.MM\\nPDF URL: http://arxiv.org/pdf/2505.23727v1\\narXiv URL: http://arxiv.org/abs/2505.23727v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e224b7f-8db6-464a-81e7-b33b2e93f085', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: FMG-Det: Foundation Model Guided Robust Object Detection\\nAuthors: Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins\\nSummary: Collecting high quality data for object detection tasks is challenging due to\\nthe inherent subjectivity in labeling the boundaries of an object. This makes\\nit difficult to not only collect consistent annotations across a dataset but\\nalso to validate them, as no two annotators are likely to label the same object\\nusing the exact same coordinates. These challenges are further compounded when\\nobject boundaries are partially visible or blurred, which can be the case in\\nmany domains. Training on noisy annotations significantly degrades detector\\nperformance, rendering them unusable, particularly in few-shot settings, where\\njust a few corrupted annotations can impact model performance. In this work, we\\npropose FMG-Det, a simple, efficient methodology for training models with noisy\\nannotations. More specifically, we propose combining a multiple instance\\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\\nfoundation models to correct labels prior to training. This pre-processing\\npipeline, along with slight modifications to the detector head, results in\\nstate-of-the-art performance across a number of datasets, for both standard and\\nfew-shot scenarios, while being much simpler and more efficient than other\\napproaches.\\nPublished: 2025-05-29 17:55:41+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23726v1\\narXiv URL: http://arxiv.org/abs/2505.23726v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7b73f84-d90a-4aa6-a078-30f903f24e8e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: MuLoCo: Muon is a practical inner optimizer for DiLoCo\\nAuthors: Benjamin Thérien, Xiaolong Huang, Irina Rish, Eugene Belilovsky\\nSummary: DiLoCo is a powerful framework for training large language models (LLMs)\\nunder networking constraints with advantages for increasing parallelism and\\naccelerator utilization in data center settings. Despite significantly reducing\\ncommunication frequency, however, DiLoCo's communication steps still involve\\nall-reducing a complete copy of the model's parameters. While existing works\\nhave explored ways to reduce communication in DiLoCo, the role of error\\nfeedback accumulators and the effect of the inner-optimizer on compressibility\\nremain under-explored. In this work, we investigate the effectiveness of\\nstandard compression methods including Top-k sparsification and quantization\\nfor reducing the communication overhead of DiLoCo when paired with two local\\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\\ntransformer language models (LMs) reveal that leveraging Muon as the inner\\noptimizer for DiLoCo along with an error-feedback accumulator allows to\\naggressively compress the communicated delta to 2-bits with next to no\\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\\nsignificantly outperforms DiLoCo while communicating 8X less and having\\nidentical memory complexity.\\nPublished: 2025-05-29 17:55:37+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23725v1\\narXiv URL: http://arxiv.org/abs/2505.23725v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bf8b331-2a21-4c79-b6f8-a4c3bacaa929', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA\\nAuthors: Minrui Luo, Fuhang Kuang, Yu Wang, Zirui Liu, Tianxing He\\nSummary: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\\nknowledge forgetting problems. Recent studies have leveraged the power of\\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\\npreserve knowledge in the pre-trained LLM. However, none of these works can\\naddress the two cases at the same time. To this end, we introduce\\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\\nengineered to navigate the trade-off between efficient fine-tuning and\\nknowledge preservation. We achieve this by constraining the output of trainable\\nLoRA adapters in a low-rank subspace, where the context information of\\nfine-tuning data is most preserved while the context information of preserved\\nknowledge is least retained, in a balanced way. Such constraint enables the\\ntrainable weights to primarily focus on the main features of fine-tuning data\\nwhile avoiding damaging the preserved knowledge features. We provide\\ntheoretical analysis on our method, and conduct extensive experiments including\\nsafety preservation and world knowledge preservation, on various downstream\\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\\nperformance while markedly diminishing knowledge forgetting, surpassing\\ncontemporary LoRA initialization methods.\\nPublished: 2025-05-29 17:55:21+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23724v1\\narXiv URL: http://arxiv.org/abs/2505.23724v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d9e615a-62e7-4463-bba3-ae48b826cb9b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering\\nAuthors: Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen\\nSummary: The emergence of large language model (LLM)-based agents has significantly\\nadvanced the development of autonomous machine learning (ML) engineering.\\nHowever, most existing approaches rely heavily on manual prompt engineering,\\nfailing to adapt and optimize based on diverse experimental experiences.\\nFocusing on this, for the first time, we explore the paradigm of learning-based\\nagentic ML, where an LLM agent learns through interactive experimentation on ML\\ntasks using online reinforcement learning (RL). To realize this, we propose a\\nnovel agentic ML training framework with three key components: (1)\\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\\non a single action step, accelerating experience collection and improving\\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\\nvaried ML feedback signals into consistent rewards for RL optimization.\\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\\nachieves continuous performance improvements and demonstrates exceptional\\ncross-task generalization capabilities.\\nPublished: 2025-05-29 17:54:44+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI, cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23723v1\\narXiv URL: http://arxiv.org/abs/2505.23723v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a5adc13a-17fc-4966-8948-128c978da09c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Label-Guided In-Context Learning for Named Entity Recognition\\nAuthors: Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze\\nSummary: In-context learning (ICL) enables large language models (LLMs) to perform new\\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\\ndemonstrations are typically selected based on semantic similarity to the test\\ninstance, ignoring training labels and resulting in suboptimal performance. We\\nintroduce DEER, a new method that leverages training labels through token-level\\nstatistics to improve ICL performance. DEER first enhances example selection\\nwith a label-guided, token-based retriever that prioritizes tokens most\\ninformative for entity recognition. It then prompts the LLM to revisit\\nerror-prone tokens, which are also identified using label statistics, and make\\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\\nDEER consistently outperforms existing ICL methods and approaches the\\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\\non both seen and unseen entities and its robustness in low-resource settings.\\nPublished: 2025-05-29 17:54:32+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2505.23722v1\\narXiv URL: http://arxiv.org/abs/2505.23722v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f93808d-282b-4769-b78d-85049cf57e31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: DiffER: Categorical Diffusion for Chemical Retrosynthesis\\nAuthors: Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy\\nSummary: Methods for automatic chemical retrosynthesis have found recent success\\nthrough the application of models traditionally built for natural language\\nprocessing, primarily through transformer neural networks. These models have\\ndemonstrated significant ability to translate between the SMILES encodings of\\nchemical products and reactants, but are constrained as a result of their\\nautoregressive nature. We propose DiffER, an alternative template-free method\\nfor retrosynthesis prediction in the form of categorical diffusion, which\\nallows the entire output SMILES sequence to be predicted in unison. We\\nconstruct an ensemble of diffusion models which achieves state-of-the-art\\nperformance for top-1 accuracy and competitive performance for top-3, top-5,\\nand top-10 accuracy among template-free methods. We prove that DiffER is a\\nstrong baseline for a new class of template-free model, capable of learning a\\nvariety of synthetic techniques used in laboratory settings and outperforming a\\nvariety of other template-free methods on top-k accuracy metrics. By\\nconstructing an ensemble of categorical diffusion models with a novel length\\nprediction component with variance, our method is able to approximately sample\\nfrom the posterior distribution of reactants, producing results with strong\\nmetrics of confidence and likelihood. Furthermore, our analyses demonstrate\\nthat accurate prediction of the SMILES sequence length is key to further\\nboosting the performance of categorical diffusion models.\\nPublished: 2025-05-29 17:53:37+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23721v1\\narXiv URL: http://arxiv.org/abs/2505.23721v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8f8ceb1d-175b-4a35-8033-cc1ff6a41e3b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning\\nAuthors: Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, Sepp Hochreiter\\nSummary: In-context learning, the ability of large language models to perform tasks\\nusing only examples provided in the prompt, has recently been adapted for time\\nseries forecasting. This paradigm enables zero-shot prediction, where past\\nvalues serve as context for forecasting future values, making powerful\\nforecasting tools accessible to non-experts and increasing the performance when\\ntraining data are scarce. Most existing zero-shot forecasting approaches rely\\non transformer architectures, which, despite their success in language, often\\nfall short of expectations in time series forecasting, where recurrent models\\nlike LSTMs frequently have the edge. Conversely, while LSTMs are well-suited\\nfor time series modeling due to their state-tracking capabilities, they lack\\nstrong in-context learning abilities. We introduce TiRex that closes this gap\\nby leveraging xLSTM, an enhanced LSTM with competitive in-context learning\\nskills. Unlike transformers, state-space models, or parallelizable RNNs such as\\nRWKV, TiRex retains state-tracking, a critical property for long-horizon\\nforecasting. To further facilitate its state-tracking ability, we propose a\\ntraining-time masking strategy called CPM. TiRex sets a new state of the art in\\nzero-shot time series forecasting on the HuggingFace benchmarks GiftEval and\\nChronos-ZS, outperforming significantly larger models including TabPFN-TS\\n(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)\\nacross both short- and long-term forecasts.\\nPublished: 2025-05-29 17:52:10+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2505.23719v1\\narXiv URL: http://arxiv.org/abs/2505.23719v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bafde3e-d3f7-4384-bf41-b2b1cbbf9a3a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?\\nAuthors: Olaf Dössel, Axel Loewe\\nSummary: This review focuses on the computerized modeling of the electrophysiology of\\nthe human atria, emphasizing the simulation of common arrhythmias such as\\natrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the\\nmodel are necessary to accurately model arrhythmogenic tissue modifications,\\nincluding remodeling, cardiomyopathy, and fibrosis, to ensure reliable\\nsimulations? The central question explored is the level of detail required for\\ntrustworthy simulations for a specific context of use. The review discusses the\\nbalance between model complexity and computational efficiency, highlighting the\\nrisks of oversimplification and excessive detail. It covers various aspects of\\natrial modeling, from cellular to whole atria levels, including the influence\\nof atrial geometry, fiber direction, anisotropy, and wall thickness on\\nsimulation outcomes. The article also examines the impact of different modeling\\napproaches, such as volumetric 3D models, bilayer models, and single surface\\nmodels, on the realism of simulations. In addition, it reviews the latest\\nadvances in the modeling of fibrotic tissue and the verification and validation\\nof atrial models. The intended use of these models in planning and optimization\\nof atrial ablation strategies is discussed, with a focus on personalized\\nmodeling for individual patients and cohort-based approaches for broader\\napplications. The review concludes by emphasizing the importance of integrating\\nexperimental data and clinical validation to enhance the utility of\\ncomputerized atrial models to improve patient outcomes.\\nPublished: 2025-05-29 17:51:40+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: physics.comp-ph\\nCategories: physics.comp-ph, cs.CE\\nPDF URL: http://arxiv.org/pdf/2505.23717v1\\narXiv URL: http://arxiv.org/abs/2505.23717v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='adf96350-4967-4824-acbe-236e931f61f1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views\\nAuthors: Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai\\nSummary: We introduce AnySplat, a feed forward network for novel view synthesis from\\nuncalibrated image collections. In contrast to traditional neural rendering\\npipelines that demand known camera poses and per scene optimization, or recent\\nfeed forward methods that buckle under the computational weight of dense views,\\nour model predicts everything in one shot. A single forward pass yields a set\\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\\ncorresponding camera intrinsics and extrinsics for each input image. This\\nunified design scales effortlessly to casually captured, multi view datasets\\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\\nmatches the quality of pose aware baselines in both sparse and dense view\\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\\nreduce rendering latency compared to optimization based neural fields, bringing\\nreal time novel view synthesis within reach for unconstrained capture\\nsettings.Project page: https://city-super.github.io/anysplat/\\nPublished: 2025-05-29 17:49:56+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2505.23716v1\\narXiv URL: http://arxiv.org/abs/2505.23716v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9064fb48-a86c-43ea-aec6-519036615699', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models\\nAuthors: Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu\\nSummary: Large language models (LLMs) have witnessed rapid advancements, demonstrating\\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\\nuncritically accept flawed or contradictory premises, leading to inefficient\\nreasoning and unreliable outputs. This emphasizes the significance of\\npossessing the \\\\textbf{Premise Critique Ability} for LLMs, defined as the\\ncapacity to proactively identify and articulate errors in input premises. Most\\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\\nignoring their vulnerabilities when faced with flawed premises. Thus, we\\nintroduce the \\\\textbf{Premise Critique Bench (PCBench)}, designed by\\nincorporating four error types across three difficulty levels, paired with\\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\\nexplicit prompts to detect errors, with limited autonomous critique; (2)\\nPremise critique ability depends on question difficulty and error type, with\\ndirect contradictions being easier to detect than complex or procedural errors;\\n(3) Reasoning ability does not consistently correlate with the premise critique\\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\\nlengthening responses due to repeated attempts at resolving conflicts. These\\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\\ninput validity, positioning premise critique as a foundational capability for\\ndeveloping reliable, human-centric systems. The code is available at\\nhttps://github.com/MLGroupJLU/Premise_Critique.\\nPublished: 2025-05-29 17:49:44+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2505.23715v1\\narXiv URL: http://arxiv.org/abs/2505.23715v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f132b99e-b72c-4f30-b06e-52316964368f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods\\nAuthors: Roksana Goworek, Harpal Karlcut, Muhammad Shezad, Nijaguna Darshana, Abhishek Mane, Syam Bondada, Raghav Sikka, Ulvi Mammadov, Rauf Allahverdiyev, Sriram Purighella, Paridhi Gupta, Muhinyia Ndegwa, Haim Dubossarsky\\nSummary: This paper addresses the critical need for high-quality evaluation datasets\\nin low-resource languages to advance cross-lingual transfer. While\\ncross-lingual transfer offers a key strategy for leveraging multilingual\\npretraining to expand language technologies to understudied and typologically\\ndiverse languages, its effectiveness is dependent on quality and suitable\\nbenchmarks. We release new sense-annotated datasets of sentences containing\\npolysemous words, spanning nine low-resource languages across diverse language\\nfamilies and scripts. To facilitate dataset creation, the paper presents a\\ndemonstrably beneficial semi-automatic annotation method. The utility of the\\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\\nthat evaluate transfer on these low-resource languages. Results highlight the\\nimportance of targeted dataset creation and evaluation for effective polysemy\\ndisambiguation in low-resource settings and transfer studies. The released\\ndatasets and code aim to support further research into fair, robust, and truly\\nmultilingual NLP.\\nPublished: 2025-05-29 17:48:08+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23714v1\\narXiv URL: http://arxiv.org/abs/2505.23714v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90331dd0-2b56-45f1-b200-adf17fa2c186', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models\\nAuthors: Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen\\nSummary: Large language models (LLMs) are increasingly applied to socially grounded\\ntasks, such as online community moderation, media content analysis, and social\\nreasoning games. Success in these contexts depends on a model's social\\nreasoning ability - the capacity to interpret social contexts, infer others'\\nmental states, and assess the truthfulness of presented information. However,\\nthere is currently no systematic evaluation framework that comprehensively\\nassesses the social reasoning capabilities of LLMs. Existing efforts often\\noversimplify real-world scenarios and consist of tasks that are too basic to\\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\\nsystematically incorporates three core challenges: deep reasoning, dynamic\\ninteraction, and information uncertainty. It provides six diverse tasks across\\nthree key settings: social reasoning games, daily-life interactions, and\\ndigital community platforms. Both automated and human validation are used to\\nensure data quality. Our evaluation reveals several key insights: models vary\\nsubstantially in their ability to handle dynamic interactions and integrate\\ntemporally evolving information; models with strong chain-of-thought reasoning\\nperform better on tasks requiring deeper inference beyond surface-level cues;\\nand model reasoning degrades significantly under uncertainty. Furthermore, we\\nshow that targeted fine-tuning on curated reasoning examples can greatly\\nimprove model performance in complex social scenarios. The dataset is publicly\\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze\\nPublished: 2025-05-29 17:47:36+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2505.23713v1\\narXiv URL: http://arxiv.org/abs/2505.23713v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='771bd8c0-5cb9-4bb7-bd16-76cc5ee81f9c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Neutron Stars in Causal Scalar-Tensor Theories\\nAuthors: Mark P. Hertzberg, Oleksandr S. Stashko\\nSummary: We study static, spherically symmetric neutron stars in a class of\\nscalar-tensor theories with non-canonical kinetic terms (K-essence) obeying all\\ncausality and hyperbolicity conditions. These models have non-trivial dynamics\\nthat lead to a type of anti-screening of the scalar. They lead to small\\ncorrections in the solar system due to a small coupling, but can lead to large\\ncorrections in regimes of high densities, especially neutron stars. We solve\\nthe modified Tolman-Oppenheimer-Volkoff equations numerically using realistic\\nequations of state (SLy4, WFF1, MS1, MPA1). For a given central density, we\\nfind that two distinct configurations may exist, forming two separate branches\\nof solutions. We find that above a certain critical central density solutions\\nwith the correct asymptotic behavior at spatial infinity cannot be obtained. We\\nobtain precise predictions for the mass-radius relation for neutron stars for\\ndifferent values of the parameters in the model and we compare to data.\\nPublished: 2025-05-29 17:46:22+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: gr-qc\\nCategories: gr-qc, astro-ph.CO, hep-ph, hep-th\\nPDF URL: http://arxiv.org/pdf/2505.23712v1\\narXiv URL: http://arxiv.org/abs/2505.23712v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75f8fff1-af16-4813-88e7-cda0fdbe417c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Hub Detection in Gaussian Graphical Models\\nAuthors: José Á. Sánchez Gómez, Weibin Mo, Junlong Zhao, Yufeng Liu\\nSummary: Graphical models are popular tools for exploring relationships among a set of\\nvariables. The Gaussian graphical model (GGM) is an important class of\\ngraphical models, where the conditional dependence among variables is\\nrepresented by nodes and edges in a graph. In many real applications, we are\\ninterested in detecting hubs in graphical models, which refer to nodes with a\\nsignificant higher degree of connectivity compared to non-hub nodes. A typical\\nstrategy for hub detection consists of estimating the graphical model, and then\\nusing the estimated graph to identify hubs. Despite its simplicity, the success\\nof this strategy relies on the accuracy of the estimated graph. In this paper,\\nwe directly target on the estimation of hubs, without the need of estimating\\nthe graph. We establish a novel connection between the presence of hubs in a\\ngraphical model, and the spectral decomposition of the underlying covariance\\nmatrix. Based on this connection, we propose the method of inverse principal\\ncomponents for hub detection (IPC-HD). Both consistency and convergence rates\\nare established for IPC-HD. Our simulation study demonstrates the superior\\nperformance and fast computation of the proposed method compared to existing\\nmethods in the literature in terms of hub detection. Our application to a\\nprostate cancer gene expression dataset detects several hub genes with close\\nconnections to tumor development.\\nPublished: 2025-05-29 17:41:19+00:00\\nJournal Reference: None\\nDOI: 10.1080/01621459.2025.2453250\\nPrimary Category: stat.ME\\nCategories: stat.ME\\nPDF URL: http://arxiv.org/pdf/2505.23707v1\\narXiv URL: http://arxiv.org/abs/2505.23707v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f93d6132-0fae-4bc9-a7ba-49460b7831ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats\\nAuthors: Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, Mengran Xue\\nSummary: In connected and autonomous vehicles, machine learning for safety message\\nclassification has become critical for detecting malicious or anomalous\\nbehavior. However, conventional approaches that rely on centralized data\\ncollection or purely local training face limitations due to the large scale,\\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\\nnetworks. To overcome these challenges, this paper explores Distributed\\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\\nmodels by exchanging model updates among one-hop neighbors and propagating\\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\\nExtension Dataset, we show that DFL can significantly improve classification\\naccuracy across all vehicles compared to learning strictly with local data.\\nNotably, vehicles with low individual accuracy see substantial accuracy gains\\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\\nWe further show that local training data size and time-varying network\\nconnectivity correlate strongly with the model's overall accuracy. We\\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\\ndomains, namely wireless jamming and training data poisoning attacks. Our\\nresults reveal important insights into the vulnerabilities of DFL when\\nconfronted with multi-domain attacks, underlining the need for more robust\\nstrategies to secure DFL in vehicular networks.\\nPublished: 2025-05-29 17:41:02+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.NI\\nCategories: cs.NI, cs.AI, cs.DC, cs.IT, eess.SP, math.IT\\nPDF URL: http://arxiv.org/pdf/2505.23706v1\\narXiv URL: http://arxiv.org/abs/2505.23706v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0ef0a73a-9c2b-4148-a846-95969e54f179', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better\\nAuthors: Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, Sergey Levine\\nSummary: Vision-language-action (VLA) models provide a powerful approach to training\\ncontrol policies for physical systems, such as robots, by combining end-to-end\\nlearning with transfer of semantic knowledge from web-scale vision-language\\nmodel (VLM) training. However, the constraints of real-time control are often\\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\\nof billions of parameters, presenting an obstacle to real-time inference, and\\noperate on discrete tokens rather than the continuous-valued outputs that are\\nrequired for controlling robots. To address this challenge, recent VLA models\\nhave used specialized modules for efficient continuous control, such as action\\nexperts or continuous output heads, which typically require adding new\\nuntrained parameters to the pretrained VLM backbone. While these modules\\nimprove real-time and control capabilities, it remains an open question whether\\nthey preserve or degrade the semantic knowledge contained in the pretrained\\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\\nstudy this question in the context of VLAs that include a continuous diffusion\\nor flow matching action expert, showing that naively including such experts\\nsignificantly harms both training speed and knowledge transfer. We provide an\\nextensive analysis of various design choices, their impact on performance and\\nknowledge transfer, and propose a technique for insulating the VLM backbone\\nduring VLA training that mitigates this issue. Videos are available at\\nhttps://pi.website/research/knowledge_insulation.\\nPublished: 2025-05-29 17:40:09+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.RO\\nPDF URL: http://arxiv.org/pdf/2505.23705v1\\narXiv URL: http://arxiv.org/abs/2505.23705v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae1f7375-f500-4edb-9797-de6eaf132c1b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability\\nAuthors: Ruida Wang, Yuxin Li, Yi R., Fung, Tong Zhang\\nSummary: Enhancing the mathematical reasoning capabilities of LLMs has garnered\\nsignificant attention in both the mathematical and computer science\\ncommunities. Recent works have made substantial progress in both Natural\\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\\npotential of pure Reinforcement Learning (RL) methods on base models. However,\\nRL approaches struggle to impart new capabilities not presented in the base\\nmodel, highlighting the need to integrate more knowledge like FL into NL math\\nreasoning effectively. Yet, this integration is challenging due to inherent\\ndisparities in problem structure and reasoning format between NL and FL. To\\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\\nframework designed to incorporate the FL expert into NL math problem-solving.\\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\\nAlignment* method, which reformulates the Question-Answering (QA) problems in\\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\\ntechnique we provide enables the FL reasoner to handle both QA and existence\\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\\nexperiments demonstrate that the **HybridReasoning** framework achieves\\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\\nNotably, some problems resolved by our framework remain unsolved by the NL\\nbaseline model even under a larger number of trials.\\nPublished: 2025-05-29 17:39:30+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.AI\\nCategories: cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23703v1\\narXiv URL: http://arxiv.org/abs/2505.23703v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b5b4bb6a-358a-4114-9a9f-ad96a7751a83', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: CLDTracker: A Comprehensive Language Description for Visual Tracking\\nAuthors: Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer\\nSummary: VOT remains a fundamental yet challenging task in computer vision due to\\ndynamic appearance changes, occlusions, and background clutter. Traditional\\ntrackers, relying primarily on visual cues, often struggle in such complex\\nscenarios. Recent advancements in VLMs have shown promise in semantic\\nunderstanding for tasks like open-vocabulary detection and image captioning,\\nsuggesting their potential for VOT. However, the direct application of VLMs to\\nVOT is hindered by critical limitations: the absence of a rich and\\ncomprehensive textual representation that semantically captures the target\\nobject's nuances, limiting the effective use of language information;\\ninefficient fusion mechanisms that fail to optimally integrate visual and\\ntextual features, preventing a holistic understanding of the target; and a lack\\nof temporal modeling of the target's evolving appearance in the language\\ndomain, leading to a disconnect between the initial description and the\\nobject's subsequent visual changes. To bridge these gaps and unlock the full\\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\\nLanguage Description framework for robust visual Tracking. Our tracker\\nintroduces a dual-branch architecture consisting of a textual and a visual\\nbranch. In the textual branch, we construct a rich bag of textual descriptions\\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\\nsemantic and contextual cues to address the lack of rich textual\\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\\nCLDTracker achieves SOTA performance, validating the effectiveness of\\nleveraging robust and temporally-adaptive vision-language representations for\\ntracking. Code and models are publicly available at:\\nhttps://github.com/HamadYA/CLDTracker\\nPublished: 2025-05-29 17:39:30+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI\\nPDF URL: http://arxiv.org/pdf/2505.23704v1\\narXiv URL: http://arxiv.org/abs/2505.23704v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c4f2ab5-e0fc-458f-853b-7efd12417ac5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: (U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs\\nAuthors: Nathan Lichtlé, Alexi Canesse, Zhe Fu, Hossein Nick Zinat Matin, Maria Laura Delle Monache, Alexandre M. Bayen\\nSummary: We introduce (U)NFV, a modular neural network architecture that generalizes\\nclassical finite volume (FV) methods for solving hyperbolic conservation laws.\\nHyperbolic partial differential equations (PDEs) are challenging to solve,\\nparticularly conservation laws whose physically relevant solutions contain\\nshocks and discontinuities. FV methods are widely used for their mathematical\\nproperties: convergence to entropy solutions, flow conservation, or total\\nvariation diminishing, but often lack accuracy and flexibility in complex\\nsettings. Neural Finite Volume addresses these limitations by learning update\\nrules over extended spatial and temporal stencils while preserving conservation\\nstructure. It supports both supervised training on solution data (NFV) and\\nunsupervised training via weak-form residual loss (UNFV). Applied to\\nfirst-order conservation laws, (U)NFV achieves up to 10x lower error than\\nGodunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin\\nsolvers with far less complexity. On traffic modeling problems, both from PDEs\\nand from experimental highway data, (U)NFV captures nonlinear wave dynamics\\nwith significantly higher fidelity and scalability than traditional FV\\napproaches.\\nPublished: 2025-05-29 17:39:25+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.NA, math.NA, I.2.6; G.1.8\\nPDF URL: http://arxiv.org/pdf/2505.23702v1\\narXiv URL: http://arxiv.org/abs/2505.23702v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd6d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from constants import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e30509",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"index/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
