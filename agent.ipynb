{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76717cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from constants import embed_model\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir = \"index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9debd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from constants import llm_model\n",
    "\n",
    "query_engine = index.as_query_engine(llm_model=llm_model, similarity_top_k=5)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    name=\"research_paper_query_engine_tool\", \n",
    "    description=\"A RAG engine with recent research papers.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be357d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f\"**Prompt key**: {key}\"))\n",
    "        print(prompt.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c72b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85979a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_file_tool\",\n",
    "    description=\"python function that downloads a pdf file by link\"\n",
    ")\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_from_arxiv\",\n",
    "    description=\"download the {max_results} recent papers regarding the topic {title} from arxiv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fbe7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools([rag_tool, download_pdf_tool, fetch_arxiv_tool], llm=llm_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a85e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"I am interesting in {topic}\n",
    "Find papers in your knowledge database related to this topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to {topic}'.\n",
    "If there are not, could you fetch the recent one from arxiv?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7cae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 70fde91c-6fd4-44d6-b53a-956e6b040fc8. Step input: I am interesting in Multi-Modal Models\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'.\n",
      "If there are not, could you fetch the recent one from arxiv?\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation  \n",
      "Summary: In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation.  \n",
      "Authors: Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
      "PDF URL: http://arxiv.org/pdf/2505.23661v1\n",
      "\n",
      "Title: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos  \n",
      "Summary: MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC).  \n",
      "Authors: Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
      "PDF URL: http://arxiv.org/pdf/2505.23693v1\n",
      "\n",
      "Title: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
      "Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge.  \n",
      "Authors: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
      "PDF URL: http://arxiv.org/pdf/2505.23747v1\n",
      "\n",
      "Title: Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model  \n",
      "Summary: Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm.  \n",
      "Authors: Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan  \n",
      "PDF URL: http://arxiv.org/pdf/2505.23606v1\n",
      "\n",
      "Title: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis  \n",
      "Summary: Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis.  \n",
      "Authors: Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan  \n",
      "PDF URL: http://arxiv.org/pdf/2505.23601v1\n",
      "\u001b[0m> Running step ad00188d-eb22-43f1-b709-198f352e8d2e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Here are some recent papers related to Multi-Modal Models:\n",
      "\n",
      "1. **Title**: OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation  \n",
      "   **Summary**: In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation.  \n",
      "   **Authors**: Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
      "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23661v1)\n",
      "\n",
      "2. **Title**: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos  \n",
      "   **Summary**: MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC).  \n",
      "   **Authors**: Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
      "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23693v1)\n",
      "\n",
      "3. **Title**: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
      "   **Summary**: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge.  \n",
      "   **Authors**: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
      "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23747v1)\n",
      "\n",
      "4. **Title**: Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model  \n",
      "   **Summary**: Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm.  \n",
      "   **Authors**: Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan  \n",
      "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23606v1)\n",
      "\n",
      "5. **Title**: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis  \n",
      "   **Summary**: Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis.  \n",
      "   **Authors**: Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan  \n",
      "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23601v1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Multi-Modal Models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a10ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers related to Multi-Modal Models:\n",
       "\n",
       "1. **Title**: OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation  \n",
       "   **Summary**: In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation.  \n",
       "   **Authors**: Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
       "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23661v1)\n",
       "\n",
       "2. **Title**: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos  \n",
       "   **Summary**: MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC).  \n",
       "   **Authors**: Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
       "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23693v1)\n",
       "\n",
       "3. **Title**: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
       "   **Summary**: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge.  \n",
       "   **Authors**: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
       "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23747v1)\n",
       "\n",
       "4. **Title**: Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model  \n",
       "   **Summary**: Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm.  \n",
       "   **Authors**: Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan  \n",
       "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23606v1)\n",
       "\n",
       "5. **Title**: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis  \n",
       "   **Summary**: Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis.  \n",
       "   **Authors**: Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan  \n",
       "   **PDF URL**: [Download PDF](http://arxiv.org/pdf/2505.23601v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539afd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step ce2eeba5-c0a0-4185-9d2b-0c27ee03d72d. Step input: Download all the papers you mentioned.\n",
      "\u001b[1;3;38;5;200mThought: I need to download multiple PDF files from the provided URLs. I will do this one by one for each paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23661v1', 'output_file_name': 'OpenUni.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/OpenUni.pdf'.\n",
      "\u001b[0m> Running step f064c543-b706-4bcf-a41b-805a65b52021. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Action: download_pdf_file_tool  \n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23693v1', 'output_file_name': 'VF-Eval.pdf'}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(\"Download all the papers you mentioned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4343c4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Action: download_pdf_file_tool  \n",
       "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23693v1', 'output_file_name': 'VF-Eval.pdf'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b36b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step b3a77f3e-3fb2-4942-b1ca-8969a6d40e32. Step input: I am interesting in The history of Soccer\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to The history of Soccer'.\n",
      "If there are not, could you fetch the recent one from arxiv?\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to The history of Soccer'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: Not Found\n",
      "Summary: Not Found\n",
      "Authors: Not Found\n",
      "PDF URL: Not Found\n",
      "\u001b[0m> Running step b5062d05-ef37-4a50-8613-ec070c5327fe. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems there are no papers related to \"The history of Soccer\" in the knowledge database. I will now fetch recent papers from arxiv on this topic.\n",
      "Action: fetch_from_arxiv\n",
      "Action Input: {'title': 'The history of Soccer', 'papers_count': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: [{'title': 'TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models', 'summary': 'Image-text models excel at image-level tasks but struggle with detailed\\nvisual understanding. While these models provide strong visual-language\\nalignment, segmentation models like SAM2 offer precise spatial boundaries for\\nobjects. To this end, we propose TextRegion, a simple, effective, and\\ntraining-free framework that combines the strengths of image-text models and\\nSAM2 to generate powerful text-aligned region tokens. These tokens enable\\ndetailed visual understanding while preserving open-vocabulary capabilities.\\nThey can be directly applied to various downstream tasks, including open-world\\nsemantic segmentation, referring expression comprehension, and grounding. We\\nconduct extensive evaluations and consistently achieve superior or competitive\\nperformance compared to state-of-the-art training-free methods. Additionally,\\nour framework is compatible with many image-text models, making it highly\\npractical and easily extensible as stronger models emerge. Code is available\\nat: https://github.com/avaxiao/TextRegion.', 'published': datetime.datetime(2025, 5, 29, 17, 59, 59, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV'], 'pdf_url': 'http://arxiv.org/pdf/2505.23769v1', 'arxiv_url': 'http://arxiv.org/abs/2505.23769v1', 'authors': ['Yao Xiao', 'Qiqian Fu', 'Heyi Tao', 'Yuqun Wu', 'Zhen Zhu', 'Derek Hoiem']}, {'title': 'Turbulence in Primordial Dark Matter Halos and Its Impact on the First Star Formation', 'summary': 'We present high-resolution simulations of the first star-forming clouds in 15\\nminihalos with masses ranging from $\\\\sim 10^5$ to $10^7\\\\ \\\\text{M}_{\\\\odot}$ at\\nredshifts $z \\\\sim 17 - 20$, using the \\\\texttt{GIZMO} code. Our simulations\\nincorporate detailed primordial gas physics and adopt initial conditions from\\nthe state-of-the-art TNG cosmological simulations. To achieve the required\\nresolution, we apply a particle-splitting technique that increases the\\nresolution of the original TNG data by a factor of $\\\\sim 10^5$, reaching gas\\nand dark matter particle masses of $0.2\\\\ \\\\text{M}_{\\\\odot}$ and $80\\\\\\n\\\\text{M}_{\\\\odot}$, respectively. This enables us to resolve gas accretion\\nduring the early assembly of minihalos and to capture the emergence of strong\\nturbulent flows. We find that turbulence, driven by gas infall into the dark\\nmatter potential wells, is predominantly supersonic, with characteristic Mach\\nnumbers ranging from $1.8$ to $4.2$, increasing with halo mass. The supersonic\\nturbulence effectively fragments the central gas cloud into multiple dense\\nclumps, some of which form gravitationally bound cores and begin to collapse\\ninto the first stars. Our results suggest that supersonic turbulence is a\\ncommon feature in minihalos and plays a key role in generating clumpy\\nstar-forming clouds, with important implications for the initial mass function\\nof the first stars.', 'published': datetime.datetime(2025, 5, 29, 17, 59, 58, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'astro-ph.GA', 'categories': ['astro-ph.GA', 'astro-ph.CO'], 'pdf_url': 'http://arxiv.org/pdf/2505.23768v1', 'arxiv_url': 'http://arxiv.org/abs/2505.23768v1', 'authors': ['Meng-Yuan Ho', 'Ke-Jung Chen', 'Pei-Cheng Tung']}, {'title': 'Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought', 'summary': 'Recent advances in multimodal large language models (MLLMs) have demonstrated\\nremarkable capabilities in vision-language tasks, yet they often struggle with\\nvision-centric scenarios where precise visual focus is needed for accurate\\nreasoning. In this paper, we introduce Argus to address these limitations with\\na new visual attention grounding mechanism. Our approach employs object-centric\\ngrounding as visual chain-of-thought signals, enabling more effective\\ngoal-conditioned visual attention during multimodal reasoning tasks.\\nEvaluations on diverse benchmarks demonstrate that Argus excels in both\\nmultimodal reasoning tasks and referring object grounding tasks. Extensive\\nanalysis further validates various design choices of Argus, and reveals the\\neffectiveness of explicit language-guided visual region-of-interest engagement\\nin MLLMs, highlighting the importance of advancing multimodal intelligence from\\na visual-centric perspective. Project page: https://yunzeman.github.io/argus/', 'published': datetime.datetime(2025, 5, 29, 17, 59, 56, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV'], 'pdf_url': 'http://arxiv.org/pdf/2505.23766v1', 'arxiv_url': 'http://arxiv.org/abs/2505.23766v1', 'authors': ['Yunze Man', 'De-An Huang', 'Guilin Liu', 'Shiwei Sheng', 'Shilong Liu', 'Liang-Yan Gui', 'Jan Kautz', 'Yu-Xiong Wang', 'Zhiding Yu']}, {'title': 'On completely monotonic functions', 'summary': 'Let $ f:(0,\\\\infty)\\\\rightarrow \\\\Bbb{R} $ be a completely monotonic function.\\nIn this paper, we present some properties of this functions and several new\\nclasses of completely monotonic functions. We also give some special functions\\nsuch that its have completely monotonic condition.', 'published': datetime.datetime(2025, 5, 29, 17, 59, 56, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'math.CA', 'categories': ['math.CA'], 'pdf_url': 'http://arxiv.org/pdf/2505.23767v1', 'arxiv_url': 'http://arxiv.org/abs/2505.23767v1', 'authors': ['Mostafa Najafi', 'Ali Morassaei']}, {'title': 'From Chat Logs to Collective Insights: Aggregative Question Answering', 'summary': 'Conversational agents powered by large language models (LLMs) are rapidly\\nbecoming integral to our daily interactions, generating unprecedented amounts\\nof conversational data. Such datasets offer a powerful lens into societal\\ninterests, trending topics, and collective concerns. Yet, existing approaches\\ntypically treat these interactions as independent and miss critical insights\\nthat could emerge from aggregating and reasoning across large-scale\\nconversation logs. In this paper, we introduce Aggregative Question Answering,\\na novel task requiring models to reason explicitly over thousands of\\nuser-chatbot interactions to answer aggregative queries, such as identifying\\nemerging concerns among specific demographics. To enable research in this\\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\\nquestions derived from 182,330 real-world chatbot conversations. Experiments\\nshow that existing methods either struggle to reason effectively or incur\\nprohibitive computational costs, underscoring the need for new approaches\\ncapable of extracting collective insights from large-scale conversational data.', 'published': datetime.datetime(2025, 5, 29, 17, 59, 55, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.LG'], 'pdf_url': 'http://arxiv.org/pdf/2505.23765v1', 'arxiv_url': 'http://arxiv.org/abs/2505.23765v1', 'authors': ['Wentao Zhang', 'Woojeong Kim', 'Yuntian Deng']}]\n",
      "\u001b[0m> Running step 3db3a02d-8928-44bc-b7fc-c41e53f53bff. Step input: None\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 936c1d50-2b8c-4233-955b-5722ffdb46fb. Step input: None\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 013753d4-fe3c-4919-95dc-1c074fb47280. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems that there are no relevant papers specifically about \"The history of Soccer\" from arxiv either. I cannot provide any papers on this topic.\n",
      "Answer: Unfortunately, I couldn't find any research papers related to \"The history of Soccer\" in the available databases.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"The history of Soccer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f661e26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I couldn't find any research papers related to \"The history of Soccer\" in the available databases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
