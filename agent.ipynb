{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76717cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from constants import embed_model\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir = \"index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9debd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from constants import llm_model\n",
    "\n",
    "query_engine = index.as_query_engine(llm_model=llm_model, similarity_top_k=5)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    name=\"research_paper_query_engine_tool\", \n",
    "    description=\"A RAG engine with recent research papers.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be357d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f\"**Prompt key**: {key}\"))\n",
    "        print(prompt.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6c72b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85979a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_file_tool\",\n",
    "    description=\"python function that downloads a pdf file by link\"\n",
    ")\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_from_arxiv\",\n",
    "    description=\"download the {max_results} recent papers regarding the topic {title} from arxiv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85fbe7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools([rag_tool, download_pdf_tool, fetch_arxiv_tool], llm=llm_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56a85e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"I am interesting in {topic}\n",
    "Find papers in your knowledge database related to this topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to {topic}'.\n",
    "If there are not, could you fetch the recent one from arxiv?\n",
    "IMPORTANT: do not download papers unless the user ask for it explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7cae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e48990a3-907a-4321-a208-3e5d39099b77. Step input: I am interesting in Multi-Modal Models\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'.\n",
      "If there are not, could you fetch the recent one from arxiv?\n",
      "IMPORTANT: do not download papers unless the user ask for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me find papers related to Multi-Modal Models.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation\n",
      "Authors: Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy\n",
      "Summary: In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector.\n",
      "PDF URL: http://arxiv.org/pdf/2505.23661v1\n",
      "\n",
      "Title: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos\n",
      "Authors: Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao\n",
      "Summary: MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos.\n",
      "PDF URL: http://arxiv.org/pdf/2505.23693v1\n",
      "\n",
      "Title: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence\n",
      "Authors: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan\n",
      "Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations.\n",
      "PDF URL: http://arxiv.org/pdf/2505.23747v1\n",
      "\u001b[0m> Running step 7e751473-41b2-46ef-81a4-d5a0f0381655. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have found relevant papers related to Multi-Modal Models. I will summarize them for the user.\n",
      "Answer: Here are some recent papers related to Multi-Modal Models:\n",
      "\n",
      "1. **Title:** OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation  \n",
      "   **Authors:** Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
      "   **Summary:** In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector.  \n",
      "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23661v1)\n",
      "\n",
      "2. **Title:** VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos  \n",
      "   **Authors:** Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
      "   **Summary:** MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos.  \n",
      "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23693v1)\n",
      "\n",
      "3. **Title:** Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
      "   **Authors:** Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
      "   **Summary:** Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations.  \n",
      "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23747v1)\n",
      "\n",
      "If you would like to download any of these papers, please let me know!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Multi-Modal Models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8a10ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers related to Multi-Modal Models:\n",
       "\n",
       "1. **Title:** OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation  \n",
       "   **Authors:** Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
       "   **Summary:** In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector.  \n",
       "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23661v1)\n",
       "\n",
       "2. **Title:** VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos  \n",
       "   **Authors:** Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
       "   **Summary:** MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos.  \n",
       "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23693v1)\n",
       "\n",
       "3. **Title:** Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
       "   **Authors:** Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
       "   **Summary:** Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations.  \n",
       "   **PDF URL:** [Download PDF](http://arxiv.org/pdf/2505.23747v1)\n",
       "\n",
       "If you would like to download any of these papers, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539afd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e860cfd6-0392-410a-86ca-4ffec3522a64. Step input: Download the following papers: \n",
      "                    1. Process one paper at the time.\n",
      "                    2. State which papaer number you are ptocessing of the tool\n",
      "                    3. Complete a full download cycle before moving to the next paper\n",
      "                    4. Explicity state when moving to the next paper\n",
      "                    5. Provide a final summary only after all papers are download\n",
      "                    \n",
      "\u001b[1;3;38;5;200mThought: I will start downloading the first paper related to Multi-Modal Models.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23661v1', 'output_file_name': 'OpenUni.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/OpenUni.pdf'.\n",
      "\u001b[0m> Running step 7c43df4d-ea71-4ab6-8cac-dd18123f5dda. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the first paper titled \"OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation.\" Now, I will proceed to download the second paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23693v1', 'output_file_name': 'VF-Eval.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/VF-Eval.pdf'.\n",
      "\u001b[0m> Running step 0f134e41-cef2-4c01-875f-f5199af020e9. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the second paper titled \"VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos.\" Now, I will proceed to download the third paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2505.23747v1', 'output_file_name': 'Spatial-MLLM.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/Spatial-MLLM.pdf'.\n",
      "\u001b[0m> Running step 4c14bc31-a49c-4dc6-af95-f3fb2af598ef. Step input: None\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step ee07ab02-1336-48a2-9bc8-da8413d3e7a5. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the third paper titled \"Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence.\" Now that all papers have been downloaded, I will provide a final summary of the papers.\n",
      "Answer: 1. **OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation**  \n",
      "   - **Authors:** Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
      "   - **Summary:** This paper presents OpenUni, a lightweight and open-source baseline for unifying multimodal understanding and generation, utilizing a training strategy that bridges multimodal large language models and diffusion models.\n",
      "\n",
      "2. **VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**  \n",
      "   - **Authors:** Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
      "   - **Summary:** This work introduces VF-Eval, a benchmark for evaluating multimodal large language models on AI-generated content videos, focusing on tasks like coherence validation and reasoning evaluation.\n",
      "\n",
      "3. **Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence**  \n",
      "   - **Authors:** Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
      "   - **Summary:** The paper presents Spatial-MLLM, a framework for visual-based spatial reasoning from 2D observations, addressing the challenge of improving spatial intelligence in multimodal large language models.\n",
      "\n",
      "All papers have been successfully downloaded and summarized. If you need further assistance, feel free to ask!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(\"\"\"Download the following papers: \n",
    "                    1. Process one paper at the time.\n",
    "                    2. State which papaer number you are ptocessing of the tool\n",
    "                    3. Complete a full download cycle before moving to the next paper\n",
    "                    4. Explicity state when moving to the next paper\n",
    "                    5. Provide a final summary only after all papers are download\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4343c4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation**  \n",
       "   - **Authors:** Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy  \n",
       "   - **Summary:** This paper presents OpenUni, a lightweight and open-source baseline for unifying multimodal understanding and generation, utilizing a training strategy that bridges multimodal large language models and diffusion models.\n",
       "\n",
       "2. **VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**  \n",
       "   - **Authors:** Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao  \n",
       "   - **Summary:** This work introduces VF-Eval, a benchmark for evaluating multimodal large language models on AI-generated content videos, focusing on tasks like coherence validation and reasoning evaluation.\n",
       "\n",
       "3. **Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence**  \n",
       "   - **Authors:** Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
       "   - **Summary:** The paper presents Spatial-MLLM, a framework for visual-based spatial reasoning from 2D observations, addressing the challenge of improving spatial intelligence in multimodal large language models.\n",
       "\n",
       "All papers have been successfully downloaded and summarized. If you need further assistance, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b36b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 08b61ef7-9132-4440-8c30-ebe63f397cf1. Step input: I am interesting in The history of Soccer\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to The history of Soccer'.\n",
      "If there are not, could you fetch the recent one from arxiv?\n",
      "IMPORTANT: do not download papers unless the user ask for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to The history of Soccer'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: Not Found\n",
      "Authors: Not Found\n",
      "Summary: Not Found\n",
      "PDF URL: Not Found\n",
      "\u001b[0m> Running step 725db800-42ed-44cd-891d-c75b00360d2b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems there are no papers available in the knowledge database related to the history of soccer. I will now fetch recent papers from arXiv on this topic.\n",
      "Action: fetch_from_arxiv\n",
      "Action Input: {'title': 'The history of Soccer', 'papers_count': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: [{'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents', 'summary': 'CAPTCHAs have been a critical bottleneck for deploying web agents in\\nreal-world applications, often blocking them from completing end-to-end\\nautomation tasks. While modern multimodal LLM agents have demonstrated\\nimpressive performance in static perception tasks, their ability to handle\\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\\nbenchmark and platform specifically designed to evaluate the visual reasoning\\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\\nwhich quantifies the number of cognitive and motor steps required to solve each\\npuzzle. Experimental results show that humans consistently achieve near-perfect\\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\\nthe limits of current multimodal agents and guiding the development of more\\nrobust multimodal reasoning systems. Code and Data are available at this https\\nURL.', 'published': datetime.datetime(2025, 5, 30, 17, 59, 55, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG'], 'pdf_url': 'http://arxiv.org/pdf/2505.24878v1', 'arxiv_url': 'http://arxiv.org/abs/2505.24878v1', 'authors': ['Yaxin Luo', 'Zhaoyi Li', 'Jiacheng Liu', 'Jiacheng Cui', 'Xiaohan Zhao', 'Zhiqiang Shen']}, {'title': 'AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion', 'summary': 'Existing methods for image-to-3D avatar generation struggle to produce highly\\ndetailed, animation-ready avatars suitable for real-world applications. We\\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\\ncompositional 3DGS refinement module that enhances the details of local body\\nparts through image-to-image refinement and seamlessly integrates them using a\\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\\navatars with minimal self-occlusion, enabling rigging and animation with any\\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\\nin both avatar reconstruction and reposing. Code and models will be publicly\\navailable for research purposes.', 'published': datetime.datetime(2025, 5, 30, 17, 59, 54, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV'], 'pdf_url': 'http://arxiv.org/pdf/2505.24877v1', 'arxiv_url': 'http://arxiv.org/abs/2505.24877v1', 'authors': ['Yangyi Huang', 'Ye Yuan', 'Xueting Li', 'Jan Kautz', 'Umar Iqbal']}, {'title': 'Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks', 'summary': 'Deep reasoning is fundamental for solving complex tasks, especially in\\nvision-centric scenarios that demand sequential, multimodal understanding.\\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\\nsingle-turn queries, limited visual modalities, and lack a framework to assess\\nreasoning quality over multiple steps as required in real-world settings. To\\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\\ncontexts, including images, multi-image comparisons, videos, and instructional\\ntext. These tasks span six major agentic environments: general visual\\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\\nand math reasoning. Our benchmark requires agents to integrate tool use with\\nexplicit, stepwise decision-making in these diverse settings. In addition, we\\npropose a fine-grained, step-level evaluation framework that assesses the\\ncorrectness and logical coherence of each reasoning step and the effectiveness\\nof tool usage throughout the task. Our results reveal that even the\\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\\ncapabilities and identify future research directions in vision-centric agentic\\nreasoning models. Our data and code are publicly available at\\nhttps://github.com/mbzuai-oryx/Agent-X', 'published': datetime.datetime(2025, 5, 30, 17, 59, 53, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.CL'], 'pdf_url': 'http://arxiv.org/pdf/2505.24876v1', 'arxiv_url': 'http://arxiv.org/abs/2505.24876v1', 'authors': ['Tajamul Ashraf', 'Amal Saqib', 'Hanan Ghani', 'Muhra AlMahri', 'Yuhao Li', 'Noor Ahsan', 'Umair Nawaz', 'Jean Lahoud', 'Hisham Cholakkal', 'Mubarak Shah', 'Philip Torr', 'Fahad Shahbaz Khan', 'Rao Muhammad Anwer', 'Salman Khan']}, {'title': 'ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL', 'summary': 'Although chain-of-thought reasoning and reinforcement learning (RL) have\\ndriven breakthroughs in NLP, their integration into generative vision models\\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\\nfirst imbues an autoregressive image generator with explicit text-based\\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\\ndataset of written rationales, and then refines its outputs using Group\\nRelative Policy Optimization. To enable the model to reason through text before\\ngenerating images, We automatically generate and release a corpus of model\\ncrafted rationales paired with visual prompts, enabling controlled planning of\\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\\nsignals from a pretrained vision language model to assess overall visual\\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.', 'published': datetime.datetime(2025, 5, 30, 17, 59, 48, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.CL'], 'pdf_url': 'http://arxiv.org/pdf/2505.24875v1', 'arxiv_url': 'http://arxiv.org/abs/2505.24875v1', 'authors': ['Yu Zhang', 'Yunqi Li', 'Yifan Yang', 'Rui Wang', 'Yuqing Yang', 'Dai Qi', 'Jianmin Bao', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu']}, {'title': 'The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models', 'summary': 'Neuro-symbolic learning was proposed to address challenges with training\\nneural networks for complex reasoning tasks with the added benefits of\\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\\ntraditionally train neural models in conjunction with symbolic programs, but\\nthey face significant challenges that limit them to simplistic problems. On the\\nother hand, purely-neural foundation models now reach state-of-the-art\\nperformance through prompting rather than training, but they are often\\nunreliable and lack interpretability. Supplementing foundation models with\\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\\nuse these models for complex reasoning tasks. Doing so raises the question:\\nWhat role does specialized model training as part of neuro-symbolic learning\\nhave in the age of foundation models? To explore this question, we highlight\\nthree pitfalls of traditional neuro-symbolic learning with respect to the\\ncompute, data, and programs leading to generalization problems. This position\\npaper argues that foundation models enable generalizable neuro-symbolic\\nsolutions, offering a path towards achieving the original goals of\\nneuro-symbolic learning without the downsides of training from scratch.', 'published': datetime.datetime(2025, 5, 30, 17, 59, 46, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG'], 'pdf_url': 'http://arxiv.org/pdf/2505.24874v1', 'arxiv_url': 'http://arxiv.org/abs/2505.24874v1', 'authors': ['Adam Stein', 'Aaditya Naik', 'Neelay Velingker', 'Mayur Naik', 'Eric Wong']}]\n",
      "\u001b[0m> Running step cd7e2d14-07ec-4881-ad8f-fef0d2af9154. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It appears that there are no recent papers specifically related to the history of soccer from arXiv. The fetched papers are focused on various topics in computer science and multimodal learning. Since there are no relevant papers, I cannot provide any further information on this topic.\n",
      "Answer: Unfortunately, I could not find any papers related to the history of soccer. If you have any other topics in mind or need assistance with something else, please let me know!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"The history of Soccer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f661e26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I could not find any papers related to the history of soccer. If you have any other topics in mind or need assistance with something else, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
