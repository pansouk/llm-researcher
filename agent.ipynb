{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76717cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from constants import embed_model\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir = \"index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9debd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from constants import llm_model\n",
    "\n",
    "query_engine = index.as_query_engine(llm_model=llm_model, similarity_top_k=5)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    name=\"research_paper_query_engine_tool\", \n",
    "    description=\"A RAG engine with recent research papers.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be357d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f\"**Prompt key**: {key}\"))\n",
    "        print(prompt.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c72b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85979a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_file_tool\",\n",
    "    description=\"python function that downloads a pdf file by link\"\n",
    ")\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_from_arxiv\",\n",
    "    description=\"download the {max_results} recent papers regarding the topic {title} from arxiv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fbe7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools([rag_tool, download_pdf_tool, fetch_arxiv_tool], llm=llm_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a85e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"I am interesting in {topic}\n",
    "Find papers in your knowledge database related to this topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to {topic}'.\n",
    "If there are not, could you fetch the recent one from arxiv?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7cae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step facf5855-34b0-44f5-816d-b3a3076ae88d. Step input: I am interesting in Multi-Modal Models\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'.\n",
      "If there are not, could you fetch the recent one from arxiv?\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence\n",
      "Authors: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan\n",
      "Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.\n",
      "PDF URL: http://arxiv.org/pdf/2505.23747v1\n",
      "\n",
      "Title: To Trust Or Not To Trust Your Vision-Language Model's Prediction\n",
      "Authors: Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink\n",
      "Summary: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.\n",
      "PDF URL: http://arxiv.org/pdf/2505.23745v1\n",
      "\u001b[0m> Running step d6940a61-4e3e-4085-9b62-0f809696c898. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Here are some recent papers related to Multi-Modal Models:\n",
      "\n",
      "1. **Title**: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
      "   **Authors**: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
      "   **Summary**: This paper presents Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. It proposes a dual-encoder architecture to enhance spatial understanding and introduces a space-aware frame sampling strategy for improved performance in visual-based spatial reasoning tasks.  \n",
      "   **Link to Download**: [PDF](http://arxiv.org/pdf/2505.23747v1)  \n",
      "   **Project Page**: [Spatial-MLLM Project](https://diankun-wu.github.io/Spatial-MLLM/)\n",
      "\n",
      "2. **Title**: To Trust Or Not To Trust Your Vision-Language Model's Prediction  \n",
      "   **Authors**: Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink  \n",
      "   **Summary**: This work introduces TrustVLM, a framework designed to estimate the trustworthiness of predictions made by Vision-Language Models (VLMs). It proposes a confidence-scoring function to improve misclassification detection and demonstrates significant performance improvements across various datasets.  \n",
      "   **Link to Download**: [PDF](http://arxiv.org/pdf/2505.23745v1)  \n",
      "\n",
      "These papers explore advancements in Multi-Modal Models and their applications in visual and language understanding.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Multi-Modal Models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8a10ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers related to Multi-Modal Models:\n",
       "\n",
       "1. **Title**: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence  \n",
       "   **Authors**: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan  \n",
       "   **Summary**: This paper presents Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. It proposes a dual-encoder architecture to enhance spatial understanding and introduces a space-aware frame sampling strategy for improved performance in visual-based spatial reasoning tasks.  \n",
       "   **Link to Download**: [PDF](http://arxiv.org/pdf/2505.23747v1)  \n",
       "   **Project Page**: [Spatial-MLLM Project](https://diankun-wu.github.io/Spatial-MLLM/)\n",
       "\n",
       "2. **Title**: To Trust Or Not To Trust Your Vision-Language Model's Prediction  \n",
       "   **Authors**: Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink  \n",
       "   **Summary**: This work introduces TrustVLM, a framework designed to estimate the trustworthiness of predictions made by Vision-Language Models (VLMs). It proposes a confidence-scoring function to improve misclassification detection and demonstrates significant performance improvements across various datasets.  \n",
       "   **Link to Download**: [PDF](http://arxiv.org/pdf/2505.23745v1)  \n",
       "\n",
       "These papers explore advancements in Multi-Modal Models and their applications in visual and language understanding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
